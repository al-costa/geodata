{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script ETL o fato FLUXO\n",
    "*autor: André Costa (2019-03-09)* [https://www.linkedin.com/in/a-l-costa]\n",
    "\n",
    "- Este script executa o processo ETL para gerar a tabela de fato FLUXO\n",
    "- *AVISO:* Este script substitui completamente os dados armazenados anteriormente pelos novos dados, caso haja conflito de chave única"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import UserDefinedFunction, countDistinct\n",
    "\n",
    "spark.sql(\"use geodata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extração do arquivo *eventos_de_fluxo.csv*\n",
    "- O arquivo lido gera um Spark Dataframe, que é imediatamente registrado no Hive com nome **raw_fluxo**\n",
    "- *AVISO:* Este script não prevê descomprimir o arquivo de dados sendo carregado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contagem de registros:  248589\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(when=datetime.datetime(2017, 7, 27, 9, 51, 2), concorrente_id=650509405109544),\n",
       " Row(when=datetime.datetime(2017, 6, 24, 14, 0, 26, 405000), concorrente_id=650509405109544),\n",
       " Row(when=datetime.datetime(2017, 7, 6, 21, 51, 11, 56000), concorrente_id=650509405109544)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawFluxoSchema = StructType([\n",
    "             StructField('codigo', StringType()),\n",
    "             StructField('when', TimestampType()),\n",
    "             StructField('concorrente_id', LongType()),\n",
    "            ])\n",
    "rawFluxo = spark.read.csv(\"file:/home/hadoop/data/eventos_de_fluxo.csv\",\n",
    "                          schema = rawFluxoSchema, header = True).drop('codigo')\n",
    "rawFluxo.registerTempTable('raw_fluxo')\n",
    "print('Contagem de registros: ', rawFluxo.count())\n",
    "rawFluxo.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geração do ID referente ao CALENDARIO\n",
    "- Neste etapa, definimos uma função customizada para definir o ID do CALENDÁRIO sem a necessidade de realizar uma consulta ao banco de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID para a data 2017-07-27 18:00:2 --> 19253\n",
      "ID armazenado na dimesão calendario\n",
      "+-----+\n",
      "|   id|\n",
      "+-----+\n",
      "|19253|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "BIG_BANG = pd.Timestamp('2000-01-01')\n",
    "manha = pd.Timedelta('12 hours')\n",
    "tarde = pd.Timedelta('18 hours')\n",
    "\n",
    "def get_calendar_id(ts):\n",
    "    elapsed_time = (ts - BIG_BANG)\n",
    "    DAYS = elapsed_time.days\n",
    "    intraday_timedelta = (elapsed_time - pd.Timedelta(days=DAYS))\n",
    "    if (intraday_timedelta>tarde):\n",
    "        periodo = 2\n",
    "    elif (intraday_timedelta>manha):\n",
    "        periodo = 1\n",
    "    else:\n",
    "        periodo = 0\n",
    "    return DAYS*3 + periodo\n",
    "\n",
    "print('ID para a data 2017-07-27 18:00:2 -->', get_calendar_id(pd.Timestamp('2017-07-27 18:00:2')))\n",
    "print('ID armazenado na dimesão calendario')\n",
    "spark.sql('select id FROM calendario WHERE ano=2017 and mes=7 and dia=27 and periodo=2').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geração do ID CALENDARIO e contagem de ocorrencias repetidas\n",
    "- Nesta etapa, a função de ID do calendário é aplicada, também é realizada a agregação de chaves unicas repetidas, neste caso, composta dos atributos *calendario_id* e *concorrente_id*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contagem de registros:  40215\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(when=19243, concorrente_id=557361084467209, count=1),\n",
       " Row(when=19237, concorrente_id=1637714039783369, count=1),\n",
       " Row(when=19191, concorrente_id=133752033461795, count=5)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calendar_id = UserDefinedFunction(get_calendar_id, LongType())\n",
    "indexedFluxo = rawFluxo.withColumn('when', calendar_id(rawFluxo.when)) \\\n",
    "    .groupBy('when', 'concorrente_id').count()\n",
    "\n",
    "indexedFluxo.registerTempTable('indexed_fluxo')\n",
    "\n",
    "print('Contagem de registros: ', indexedFluxo.count())\n",
    "indexedFluxo.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ligação com a dimensão CONCORRENTE\n",
    "- Neste etapa, os eventos de fluxo são conectados à dimensão CONCORRENTE (e à mini-dimensão BAIRRO), obtendo as respectivas SK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('create TEMPORARY view linked_fluxo AS \\\n",
    "    select a.when as calendario_id, b.id as concorrente_id, c.id as bairro_id, a.count as ocorrencias \\\n",
    "    FROM indexed_fluxo as a INNER join concorrente as b ON a.concorrente_id=b.codigo \\\n",
    "    INNER join bairro as c ON b.bairro_id=c.codigo and isnull(c.end_date)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregamento dos eventos de FLUXO\n",
    "- Nesta etapa, todos os registros do warehouse são combinados com os novos registros em uma operação de *FULL JOIN*\n",
    "- Então, para cada atributo, é escolhido o primeiro valor não nulo, iniciando pelos novos registros\n",
    "- Finalmente, os dados são carregados em uma tabela *buffer*, a tabela original é removida, e a tabela buffer é renomeada como definitiva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contagem de registros: 40215\n",
      "+--------------+----------+-------------+-----------+\n",
      "|concorrente_id| bairro_id|calendario_id|ocorrencias|\n",
      "+--------------+----------+-------------+-----------+\n",
      "|   -1286822256|-455108858|        19102|          2|\n",
      "|    1099627934|-636139672|        19105|          1|\n",
      "|    -250035517| -12108643|        19145|          1|\n",
      "|   -2113182782| -12108643|        19149|          1|\n",
      "|     779331024|-636139672|        19155|          3|\n",
      "+--------------+----------+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "finalFluxo = spark.sql('select NVL(a.concorrente_id, b.concorrente_id) as concorrente_id, \\\n",
    "    NVL(a.bairro_id, b.bairro_id) as bairro_id, \\\n",
    "    NVL(a.calendario_id, b.calendario_id) as calendario_id, \\\n",
    "    NVL(a.ocorrencias, b.ocorrencias) as ocorrencias \\\n",
    "    FROM linked_fluxo as a FULL join fluxo as b ON a.calendario_id=b.calendario_id \\\n",
    "    and a.concorrente_id=b.concorrente_id')\n",
    "finalFluxo.registerTempTable('final_fluxo')\n",
    "\n",
    "print('Contagem de registros:', finalFluxo.count())\n",
    "\n",
    "spark.sql('create table fluxo_buff like fluxo')\n",
    "spark.sql('insert into table fluxo_buff SELECT * from final_fluxo')\n",
    "spark.sql('drop table fluxo')\n",
    "spark.sql('alter table fluxo_buff RENAME TO fluxo')\n",
    "spark.sql('select * from fluxo limit 5').show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
