{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script para gerar a dimensão CALENDARIO\n",
    "*autor: André Costa (2019-03-09)* [https://www.linkedin.com/in/a-l-costa]\n",
    "\n",
    "\n",
    "- Data de início: 2000\n",
    "    - https://www.pcworld.com/article/2000276/a-brief-history-of-gps.html : segundo este artigo, o primeiro dispositivo móvel com serviço de GPS commercial foi o aparelho Benefon Esc!, lançado em 1999 na Europa. Porém, apenas em 2000 o departamento de defesa (dos EUA) removeu uma restrição de degradação do sinal de GPS, que fora implementada por interesse militar, possibilitando o desenvolvimento da tecnologia. Portanto, em uma estimativa conservadora, considerando também o cenário de desenvolvimento da tecnologia na localidade escopo da aplicação, acredita-se que a chance de haver um evento de fluxo com data anterior ao ano 2000 é ínfima.\n",
    "\n",
    "- Data de expiração: 2030\n",
    "    - Embora a data de expiração do calendário seja consideravelmente curta, com aproximadamente 10 anos de vida útil, esta dimensão pode ser facilmente atualizada em qualquer momento, com custo irrelevante. Recomendamos disparar um evento, com 30 dias de antecedência, pelos processos internos da empresa, para notificar sobre a necessidade de executar esta tarefa, caso o sistema esteja ainda em operação.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gênesis\n",
    "\n",
    "- Esta etapa realiza a geração dos dados que irão popular a **dimensão calendário**, utilizando uma função personalizada e uma serie de execuções dentro do contexto SPARK\n",
    "    1. Uma sequencia de datas é gerada utilizando a biblioteca Pandas\n",
    "    2. A sequência de datas é manipulada para gerar os registros, por meio da função *gen_calendar_records*, onde para cada dia na sequência de dados, três registros da **dimensão calendário** são gerados, uma para cada período do dia [manhã, tarde e noite]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(ano=2000, dia=1, dia_semana=6, id=0, mes=1, periodo=0), Row(ano=2000, dia=1, dia_semana=6, id=1, mes=1, periodo=1), Row(ano=2000, dia=1, dia_semana=6, id=2, mes=1, periodo=2), Row(ano=2000, dia=2, dia_semana=0, id=3, mes=1, periodo=0), Row(ano=2000, dia=2, dia_semana=0, id=4, mes=1, periodo=1)]\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql as pysql\n",
    "import pandas as pd\n",
    "\n",
    "translation = {'Sunday': 0, \n",
    "               'Monday': 1,\n",
    "               'Tuesday': 2,\n",
    "               'Wednesday': 3,\n",
    "               'Thursday': 4,\n",
    "               'Friday': 5,\n",
    "               'Saturday': 6,\n",
    "              }\n",
    "\n",
    "start_date = '2000-01-01'\n",
    "BIG_BANG = pd.Timestamp(start_date)\n",
    "def gen_calendar_records(date):\n",
    "    dt = pd.Timestamp(date)\n",
    "    days_offset = (dt - BIG_BANG).days * 3\n",
    "    rows = []\n",
    "    for i in range(3):\n",
    "        rows.append(pysql.Row(id=days_offset+i, ano=dt.year, mes=dt.month,\n",
    "                              dia=dt.day, periodo=i, dia_semana=translation[dt.day_name()]))\n",
    "    return rows\n",
    "\n",
    "end_date = '2029-12-31'\n",
    "\n",
    "# Gera sequência de datas\n",
    "dates = pd.date_range(start_date, end_date)\n",
    "\n",
    "seed_rdd = sc.parallelize(dates)\n",
    "\n",
    "# Aplica a transformação que cria os registros do calendário\n",
    "gen_rdd = seed_rdd.flatMap(gen_calendar_records)\n",
    "print(gen_rdd.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Materialização\n",
    "\n",
    "- Nesta etapa, o RDD é convertido em uma outra estrutura, o *Data-Frama*, conectando a tabela de dados ao esquema de dados, no processo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- ano: integer (nullable = true)\n",
      " |-- mes: short (nullable = true)\n",
      " |-- dia: short (nullable = true)\n",
      " |-- periodo: short (nullable = true)\n",
      " |-- dia_semana: short (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "calendarSchema = StructType([\n",
    "             StructField('id', IntegerType()),\n",
    "             StructField('ano', IntegerType()),\n",
    "             StructField('mes', ShortType()),\n",
    "             StructField('dia', ShortType()),\n",
    "             StructField('periodo', ShortType()),\n",
    "             StructField('dia_semana', ShortType())\n",
    "            ])\n",
    "\n",
    "calendar = sqlContext.createDataFrame(gen_rdd, calendarSchema)\n",
    "calendar.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persistência\n",
    "\n",
    "- Nesta etapa, os dados estruturados são armazenados dentro do sistema de arquivos do hadoop, e gerenciados pelo serviço **HIVE**, em formato de arquivo ORC\n",
    "\n",
    "    1. Este script assume que os dados serão sobrescritos, e não incrementados. Assim, a primeira etapa é limpar todos os dados da tabela usando a função *truncate*\n",
    "    2. Então, uma tabela temporária do HIVE é criada com os dados estruturados\n",
    "    3. Finalmente, todos os registros da tabela temporária são copiados para a tabela definitiva\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+---+---+-------+----------+\n",
      "|   id| ano|mes|dia|periodo|dia_semana|\n",
      "+-----+----+---+---+-------+----------+\n",
      "|15372|2014|  1| 11|      0|         6|\n",
      "|15373|2014|  1| 11|      1|         6|\n",
      "|15374|2014|  1| 11|      2|         6|\n",
      "|15393|2014|  1| 18|      0|         6|\n",
      "|15394|2014|  1| 18|      1|         6|\n",
      "|15395|2014|  1| 18|      2|         6|\n",
      "|15414|2014|  1| 25|      0|         6|\n",
      "|15415|2014|  1| 25|      1|         6|\n",
      "|15416|2014|  1| 25|      2|         6|\n",
      "|15435|2014|  2|  1|      0|         6|\n",
      "+-----+----+---+---+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('use geodata')\n",
    "spark.sql('SET hive.exec.dynamic.partition=true')\n",
    "spark.sql('SET hive.exec.dynamic.partition.mode=nonstrict')\n",
    "spark.sql('create table if not exists calendario_buff like calendario')\n",
    "calendar.createOrReplaceTempView('temp_calendar')\n",
    "spark.sql('insert into table calendario_buff PARTITION (dia_semana) (select * from temp_calendar)')\n",
    "spark.sql('drop table calendario')\n",
    "spark.sql('alter table calendario_buff RENAME TO calendario')\n",
    "spark.sql('select * from calendario limit 10').show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
