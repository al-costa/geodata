{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script para criar o esquema do data warehouse\n",
    "*autor: André Costa (2019-03-09)* [https://www.linkedin.com/in/a-l-costa]\n",
    "\n",
    "- Este script define as tabelas que são usadas para armazenar os dados após processados pelo pipeline ETL. As tabelas são armazenadas dentro do HDFS do Hadoop em forma ORC, e são gerenciadas pelo **Hive**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|databaseName|\n",
      "+------------+\n",
      "|     default|\n",
      "|     geodata|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#spark.sql(\"drop database geodata cascade\")\n",
    "spark.sql(\"create database if not exists geodata\")\n",
    "spark.sql(\"use geodata\")\n",
    "spark.sql(\"show databases\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensão CALENDARIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+---------------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                                      |comment|\n",
      "+----------------------------+---------------------------------------------------------------+-------+\n",
      "|id                          |int                                                            |null   |\n",
      "|ano                         |smallint                                                       |null   |\n",
      "|mes                         |tinyint                                                        |null   |\n",
      "|dia                         |tinyint                                                        |null   |\n",
      "|periodo                     |tinyint                                                        |null   |\n",
      "|dia_semana                  |tinyint                                                        |null   |\n",
      "|                            |                                                               |       |\n",
      "|# Detailed Table Information|                                                               |       |\n",
      "|Database                    |geodata                                                        |       |\n",
      "|Table                       |calendario                                                     |       |\n",
      "|Owner                       |hadoop                                                         |       |\n",
      "|Created Time                |Mon Mar 11 22:15:12 UTC 2019                                   |       |\n",
      "|Last Access                 |Thu Jan 01 00:00:00 UTC 1970                                   |       |\n",
      "|Created By                  |Spark 2.4.0                                                    |       |\n",
      "|Type                        |MANAGED                                                        |       |\n",
      "|Provider                    |hive                                                           |       |\n",
      "|Table Properties            |[transient_lastDdlTime=1552342512]                             |       |\n",
      "|Location                    |hdfs://localhost:8020/user/hive/warehouse/geodata.db/calendario|       |\n",
      "|Serde Library               |org.apache.hadoop.hive.ql.io.orc.OrcSerde                      |       |\n",
      "|InputFormat                 |org.apache.hadoop.hive.ql.io.orc.OrcInputFormat                |       |\n",
      "+----------------------------+---------------------------------------------------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#spark.sql('drop table calendario')\n",
    "spark.sql('create table if not exists calendario \\\n",
    "         (id int, ano smallint, mes tinyint, dia tinyint, periodo tinyint,dia_semana tinyint) \\\n",
    "         stored as ORC')\n",
    "spark.sql(\"describe formatted calendario\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (mini) Dimensão BAIRRO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+----------------------------+-------+\n",
      "|col_name                    |data_type                   |comment|\n",
      "+----------------------------+----------------------------+-------+\n",
      "|id                          |bigint                      |null   |\n",
      "|codigo                      |bigint                      |null   |\n",
      "|nome                        |string                      |null   |\n",
      "|area                        |float                       |null   |\n",
      "|populacao                   |int                         |null   |\n",
      "|dens_demo                   |float                       |null   |\n",
      "|agencias                    |smallint                    |null   |\n",
      "|empresas                    |int                         |null   |\n",
      "|empregados                  |bigint                      |null   |\n",
      "|renda                       |string                      |null   |\n",
      "|faturamento                 |float                       |null   |\n",
      "|start_date                  |timestamp                   |null   |\n",
      "|end_date                    |timestamp                   |null   |\n",
      "|                            |                            |       |\n",
      "|# Detailed Table Information|                            |       |\n",
      "|Database                    |geodata                     |       |\n",
      "|Table                       |bairro                      |       |\n",
      "|Owner                       |hadoop                      |       |\n",
      "|Created Time                |Mon Mar 11 22:15:12 UTC 2019|       |\n",
      "|Last Access                 |Thu Jan 01 00:00:00 UTC 1970|       |\n",
      "+----------------------------+----------------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#spark.sql('drop table bairro')\n",
    "spark.sql('create table if not exists bairro \\\n",
    "            (id bigint, codigo bigint, nome string, area float, populacao int, dens_demo float, \\\n",
    "            agencias smallint, empresas int, empregados bigint, renda string, \\\n",
    "            faturamento float, start_date timestamp, \\\n",
    "            end_date timestamp) stored as ORC')\n",
    "\n",
    "spark.sql('describe formatted bairro').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensão CONCORRENTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+----------------------------+-------+\n",
      "|col_name                    |data_type                   |comment|\n",
      "+----------------------------+----------------------------+-------+\n",
      "|id                          |bigint                      |null   |\n",
      "|codigo                      |bigint                      |null   |\n",
      "|nome                        |string                      |null   |\n",
      "|categoria                   |string                      |null   |\n",
      "|faixa_preco                 |int                         |null   |\n",
      "|endereco                    |string                      |null   |\n",
      "|municipio                   |string                      |null   |\n",
      "|uf                          |string                      |null   |\n",
      "|bairro_id                   |bigint                      |null   |\n",
      "|start_date                  |timestamp                   |null   |\n",
      "|end_date                    |timestamp                   |null   |\n",
      "|                            |                            |       |\n",
      "|# Detailed Table Information|                            |       |\n",
      "|Database                    |geodata                     |       |\n",
      "|Table                       |concorrente                 |       |\n",
      "|Owner                       |hadoop                      |       |\n",
      "|Created Time                |Mon Mar 11 22:15:12 UTC 2019|       |\n",
      "|Last Access                 |Thu Jan 01 00:00:00 UTC 1970|       |\n",
      "|Created By                  |Spark 2.4.0                 |       |\n",
      "|Type                        |MANAGED                     |       |\n",
      "+----------------------------+----------------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#spark.sql('drop table concorrente')\n",
    "spark.sql('create table if not exists concorrente \\\n",
    "            (id bigint, codigo bigint, nome string, categoria string, faixa_preco int, \\\n",
    "            endereco string, municipio string, uf string, bairro_id bigint, start_date timestamp, \\\n",
    "            end_date timestamp) stored as ORC')\n",
    "\n",
    "spark.sql('describe formatted concorrente').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fato FLUXO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+----------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                                 |comment|\n",
      "+----------------------------+----------------------------------------------------------+-------+\n",
      "|concorrente_id              |bigint                                                    |null   |\n",
      "|bairro_id                   |bigint                                                    |null   |\n",
      "|calendario_id               |int                                                       |null   |\n",
      "|ocorrencias                 |smallint                                                  |null   |\n",
      "|                            |                                                          |       |\n",
      "|# Detailed Table Information|                                                          |       |\n",
      "|Database                    |geodata                                                   |       |\n",
      "|Table                       |fluxo                                                     |       |\n",
      "|Owner                       |hadoop                                                    |       |\n",
      "|Created Time                |Mon Mar 11 22:15:13 UTC 2019                              |       |\n",
      "|Last Access                 |Thu Jan 01 00:00:00 UTC 1970                              |       |\n",
      "|Created By                  |Spark 2.4.0                                               |       |\n",
      "|Type                        |MANAGED                                                   |       |\n",
      "|Provider                    |hive                                                      |       |\n",
      "|Table Properties            |[transient_lastDdlTime=1552342513]                        |       |\n",
      "|Location                    |hdfs://localhost:8020/user/hive/warehouse/geodata.db/fluxo|       |\n",
      "|Serde Library               |org.apache.hadoop.hive.ql.io.orc.OrcSerde                 |       |\n",
      "|InputFormat                 |org.apache.hadoop.hive.ql.io.orc.OrcInputFormat           |       |\n",
      "|OutputFormat                |org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat          |       |\n",
      "|Storage Properties          |[serialization.format=1]                                  |       |\n",
      "+----------------------------+----------------------------------------------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#spark.sql('drop table fluxo')\n",
    "spark.sql('create table if not exists fluxo \\\n",
    "            (concorrente_id bigint, bairro_id bigint, calendario_id int, ocorrencias smallint) stored as ORC')\n",
    "\n",
    "spark.sql('describe formatted fluxo').show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
