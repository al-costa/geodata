{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script ETL para a dimensão CONCORRENTE (e BAIRRO)\n",
    "*autor: André Costa (2019-03-09)* [https://www.linkedin.com/in/a-l-costa]\n",
    "\n",
    "- Este script executa o processo ETL para gerar a dimensão CONCORRENTE, e também a mini-dimensão BAIRRO, diretamente associada\n",
    "- *AVISO:* Este script assume que os dados para gerar a dimensão (provenientes de diversos arquivos separados) contenham todos os registros necessários\n",
    "- *NOTA:* BAIRRO é uma mini-dimensão que faz parte da dimensão CONCORRENTE. Assim sendo, registro de bairros não associados aos concorrentes são ignorados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "spark.sql(\"use geodata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extração do arquivo *concorrentes.csv*\n",
    "- O arquivo lido gera um Spark Dataframe, que é imediatamente registrado no Hive com nome **raw_conc**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(codigo=431962533652067, nome='Boizão Lanches', categoria='Bar', faixa_preco=2, endereco='13190-000 Monte Mor', municipio='Monte Mor', uf='SP', bairro_id=None),\n",
       " Row(codigo=1663855903830869, nome='Bar do Serjão', categoria='Bar', faixa_preco=0, endereco='Rua das Dracenas, Americana', municipio='Americana', uf='SP', bairro_id=None),\n",
       " Row(codigo=567824576564110, nome='Recanto Do Kuca', categoria='Restaurant', faixa_preco=0, endereco='Jarinu', municipio='Jarinu', uf='SP', bairro_id=None)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawConcSchema = StructType([\n",
    "             StructField('codigo', LongType()),\n",
    "             StructField('nome', StringType()),\n",
    "             StructField('categoria', StringType()),\n",
    "             StructField('faixa_preco', IntegerType()),\n",
    "             StructField('endereco', StringType()),\n",
    "             StructField('municipio', StringType()),\n",
    "             StructField('uf', StringType()),\n",
    "             StructField('bairro_id', LongType()),\n",
    "            ])\n",
    "rawConc = spark.read.csv(\"file:/home/hadoop/data/concorrentes.csv\", schema = rawConcSchema, header = True)\n",
    "rawConc.registerTempTable('raw_conc')\n",
    "rawConc.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extração do arquivo *bairro.csv*\n",
    "- O arquivo lido gera um Spark Dataframe, que é imediatamente registrado no Hive com nome **raw_bairro**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(codigo=355620110, nome='Observatório', municipio='Valinhos', uf='SP', area=68.00090026855469),\n",
       " Row(codigo=3519071024, nome='Rp 6-24', municipio='Hortolândia', uf='SP', area=0.981768012046814),\n",
       " Row(codigo=3536505002, nome='Jardim De Itapoan', municipio='Paulínia', uf='SP', area=0.8085370063781738)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawBairroSchema = StructType([\n",
    "             StructField('codigo', LongType()),\n",
    "             StructField('nome', StringType()),\n",
    "             StructField('municipio', StringType()),\n",
    "             StructField('uf', StringType()),\n",
    "             StructField('area', FloatType()),\n",
    "            ])\n",
    "rawBairro = spark.read.csv(\"file:/home/hadoop/data/bairros.csv\", schema = rawBairroSchema, header = True)\n",
    "rawBairro.registerTempTable('raw_bairro')\n",
    "rawBairro.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extração do arquivo *populacao.json*\n",
    "- O arquivo lido gera um Spark Dataframe, que é imediatamente registrado no Hive com nome **raw_pop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(codigo=355620110, populacao=8717),\n",
       " Row(codigo=3519071024, populacao=5764),\n",
       " Row(codigo=3536505002, populacao=1195)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from pyspark.sql.functions import udf\n",
    "\n",
    "rawPopSchema = StructType([\n",
    "             StructField('codigo', LongType()),\n",
    "             StructField('populacao', IntegerType()),\n",
    "            ])\n",
    "rawPop = spark.read.json(\"file:/home/hadoop/data/populacao.json\", schema = rawPopSchema)\n",
    "#rawPop = rawPop.withColumn(\"dens_demo\", 0)\n",
    "rawPop.registerTempTable('raw_pop')\n",
    "rawPop.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extração do arquivo *potencial.csv*\n",
    "- O arquivo lido gera um Spark Dataframe, que é imediatamente registrado no Hive com nome **raw_pot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(codigo=355620110, agencias=2, empresas=73, empregados=772, renda='D', faturamento='28,467.00'),\n",
       " Row(codigo=3519071024, agencias=3, empresas=429, empregados=1004, renda='E', faturamento='2,707.20'),\n",
       " Row(codigo=3536505002, agencias=1, empresas=176, empregados=1663, renda='C', faturamento='28,580.00')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawPotSchema = StructType([\n",
    "             StructField('codigo', LongType()),\n",
    "             StructField('agencias', IntegerType()),\n",
    "             StructField('empresas', IntegerType()),\n",
    "             StructField('empregados', IntegerType()),\n",
    "             StructField('renda', StringType()),\n",
    "             StructField('faturamento', StringType()),\n",
    "            ])\n",
    "rawPot = spark.read.csv(\"file:/home/hadoop/data/potencial.csv\", schema = rawPotSchema, header = True)\n",
    "rawPot.registerTempTable('raw_pot')\n",
    "rawPot.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expansão da mini-dimensão BAIRRO\n",
    "- Nesta etapa, os dados provenientes de três arquivos diretamente associados à informação de localidade são combinados em uma única tabela\n",
    "- Adicionalmente, nesta etapa também é realizada a seleção dos registros de bairro diretamente associados aos estabelecimentos sendo processados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('create TEMPORARY view exp_bairro AS select a.codigo, \\\n",
    "    a.nome, a.area, b.populacao, c.agencias, c.empresas, c.empregados, \\\n",
    "    c.renda, CAST(REPLACE(c.faturamento, \",\", \"\") as FLOAT) as faturamento FROM \\\n",
    "    (select a.codigo, a.nome, a.area FROM raw_bairro as a WHERE a.codigo in (select bairro_id FROM \\\n",
    "    raw_conc WHERE isnotnull(bairro_id))) as a \\\n",
    "    LEFT join raw_pop as b ON a.codigo = b.codigo \\\n",
    "    LEFT join raw_pot as c ON a.codigo = c.codigo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verificação de alterações dos registros da mini-dimensão BAIRRO\n",
    "- Neste etapa, os registros comuns existentes no warehouse são obtidos e verificados quanto à atualização de atributos considerados de tipo 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('create TEMPORARY view old_bairro AS \\\n",
    "    select a.*, IF(a.populacao=b.populacao and a.agencias=b.agencias and \\\n",
    "    a.empresas=b.empresas and a.empregados=b.empregados and a.renda=b.renda \\\n",
    "    and a.faturamento=b.faturamento, 0, 1) as has_changed FROM bairro as a \\\n",
    "    INNER join exp_bairro as b ON a.codigo = b.codigo and isnull(a.end_date)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparação dos dados para inserção (BAIRRO)\n",
    "- Nesta etapa, os registros existentes no banco de dados expirados são atualizados com o preenchimento do atributo *end_date*\n",
    "- Adicionalmente, são atribuídas as chaves SK à todos os registros novos e os que foram atualizados, por meio do uso de uma função hash combinando a chave natural e a hora corrente\n",
    "- Finalmente, os registros de ambos subsets são combinados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contagem de registros: 106\n",
      "+-----------+-----------------+----------+--------+---------+------------------+--------+--------+----------+-----+-----------+-------------------+--------+\n",
      "|         id|             nome|    codigo|    area|populacao|         dens_demo|agencias|empresas|empregados|renda|faturamento|         start_date|end_date|\n",
      "+-----------+-----------------+----------+--------+---------+------------------+--------+--------+----------+-----+-----------+-------------------+--------+\n",
      "|-1123721990|        Indústria| 355620112| 19.6054|     9315|475.12419840457267|       4|     191|       476|    C|   244775.0|2019-03-14 02:08:06|    null|\n",
      "|-1419461051|          Rp 6-25|3519071025| 2.16498|    21943|10135.428808543591|       3|     406|      2600|    C|   490860.0|2019-03-14 02:08:06|    null|\n",
      "| 1127843794|   Alto Pinheiros|3536505022| 1.91767|     6820|  3556.39915054671|       4|     100|       522|    C|   183550.0|2019-03-14 02:08:06|    null|\n",
      "|-1854065138|Jardim De Itapoan|3536505002|0.808537|     1195|1477.9781142646516|       1|     176|      1663|    C|    28580.0|2019-03-14 02:08:06|    null|\n",
      "| 1872576311|           Rp 2-9|3519071009| 3.49806|    12057| 3446.767648721262|       4|     243|      2412|    C|   361725.0|2019-03-14 02:08:06|    null|\n",
      "+-----------+-----------------+----------+--------+---------+------------------+--------+--------+----------+-----+-----------+-------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#spark.sql('truncate table bairro')\n",
    "expired_bairro = spark.sql('select id, codigo, nome, area, populacao, dens_demo, agencias, \\\n",
    "    empresas, empregados, renda, faturamento, start_date, from_unixtime(unix_timestamp()) as end_date \\\n",
    "    from old_bairro WHERE has_changed=1')\n",
    "\n",
    "new_bairro = spark.sql('select IF(NVL(b.has_changed, 1)==1,hash(a.codigo, unix_timestamp()),b.id) as id, \\\n",
    "    a.nome, a.codigo, a.area, a.populacao, \\\n",
    "    IF(isnull(a.area) or a.area=0, null, a.populacao / a.area) as dens_demo, \\\n",
    "    a.agencias, a.empresas, a.empregados, a.renda, a.faturamento, \\\n",
    "    IF(NVL(b.has_changed, 1)==1, from_unixtime(unix_timestamp()), b.start_date) as start_date, \\\n",
    "    null as end_date FROM exp_bairro as a LEFT join old_bairro as b ON a.codigo=b.codigo')\n",
    "\n",
    "new_bairro = new_bairro.union(expired_bairro)\n",
    "print('Contagem de registros:', new_bairro.count())\n",
    "new_bairro.show(5)\n",
    "new_bairro.registerTempTable('new_bairro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregamento da mini-dimensão BAIRRO\n",
    "- Nesta etapa, todos os registros do warehouse são combinados com os novos registros em uma operação de *FULL JOIN*\n",
    "- Então, para cada atributo, é escolhido o primeiro valor não nulo, iniciando pelos novos registros\n",
    "- Finalmente, os dados são carregados em uma tabela *buffer*, a tabela original é removida, e a tabela buffer é renomeada como definitiva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contagem de registros: 106\n",
      "+-----------+----------+--------------------+-------+---------+---------+--------+--------+----------+-----+-----------+-------------------+--------+\n",
      "|         id|    codigo|                nome|   area|populacao|dens_demo|agencias|empresas|empregados|renda|faturamento|         start_date|end_date|\n",
      "+-----------+----------+--------------------+-------+---------+---------+--------+--------+----------+-----+-----------+-------------------+--------+\n",
      "|-1822637942|3552403004|Ar4 - Administraç...|12.2962|    56902|4627.6084|       1|     156|      1583|    C|   584850.0|2019-03-14 02:08:14|    null|\n",
      "|  653301608|3536505006|       Santa Cecília|1.00384|     1992|  1984.38|       4|     279|      3001|    D|    24965.0|2019-03-14 02:08:14|    null|\n",
      "|-1431099375|3552403003|Ar3 - Administraç...|11.3204|    41304|3648.6343|       1|     177|       234|    B|  1661520.0|2019-03-14 02:08:14|    null|\n",
      "|  573948575|3533403003|Região Administra...|2.02039|     2099|1038.9083|       0|     321|      1619|    C|    18590.0|2019-03-14 02:08:14|    null|\n",
      "|  701428958| 355620110|        Observatório|68.0009|     8717|128.18948|       2|      73|       772|    D|    28467.0|2019-03-14 02:08:14|    null|\n",
      "+-----------+----------+--------------------+-------+---------+---------+--------+--------+----------+-----+-----------+-------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_bairro = spark.sql('select NVL(a.id, b.id) as id, NVL(a.codigo, b.codigo) as codigo, \\\n",
    "    NVL(a.nome, b.nome) as nome, NVL(a.area, b.area) as area, \\\n",
    "    NVL(a.populacao, b.populacao) as populacao, NVL(a.dens_demo, b.dens_demo) as dens_demo, \\\n",
    "    NVL(a.agencias, b.agencias) as agencias, NVL(a.empresas, b.empresas) as empresas, \\\n",
    "    NVL(a.empregados, b.empregados) as empregados, NVL(a.renda, b.renda) as renda, \\\n",
    "    NVL(a.faturamento, b.faturamento) as faturamento, \\\n",
    "    NVL(a.start_date, b.start_date) as start_date, NVL(a.end_date, b.end_date) as end_date \\\n",
    "    FROM new_bairro as a FULL join bairro as b ON a.id=b.id')\n",
    "final_bairro.registerTempTable('final_bairro')\n",
    "print('Contagem de registros:', final_bairro.count())\n",
    "\n",
    "spark.sql('create table bairro_buff like bairro')\n",
    "spark.sql('insert into table bairro_buff SELECT * from final_bairro')\n",
    "spark.sql('drop table bairro')\n",
    "spark.sql('alter table bairro_buff RENAME TO bairro')\n",
    "spark.sql('select * from bairro limit 5').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verificação de alterações dos registros da dimensão CONCORRENTE\n",
    "- Neste etapa, os registros comuns existentes no warehouse são obtidos e verificados quanto à atualização de atributos considerados de tipo 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('create TEMPORARY view old_conc AS \\\n",
    "    select a.*, IF((isnull(a.faixa_preco) or a.faixa_preco=b.faixa_preco) and (isnull(a.bairro_id) or \\\n",
    "    a.bairro_id=b.bairro_id), 0, 1) as has_changed \\\n",
    "    FROM concorrente as a \\\n",
    "    INNER join raw_conc as b ON a.codigo = b.codigo and isnull(a.end_date)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparação dos dados para inserção (CONCORRENTE)\n",
    "- Nesta etapa, os registros existentes no banco de dados expirados são atualizados com o preenchimento do atributo *end_date*\n",
    "- Adicionalmente, são atribuídas as chaves SK à todos os registros novos e os que foram atualizados, por meio do uso de uma função hash combinando a chave natural e a hora corrente\n",
    "- Finalmente, os registros de ambos subsets são combinados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contagem de registros: 4202\n",
      "+-----------+---------------+--------------------+--------------------+-----------+--------------------+-----------+---+----------+-------------------+--------+\n",
      "|         id|         codigo|                nome|           categoria|faixa_preco|            endereco|  municipio| uf| bairro_id|         start_date|end_date|\n",
      "+-----------+---------------+--------------------+--------------------+-----------+--------------------+-----------+---+----------+-------------------+--------+\n",
      "|-1176100713|108804285864355|Restaurante Tratt...|   Buffet Restaurant|          3|Rua Camélias 317,...|   Holambra| SP|      null|2019-03-14 02:08:27|    null|\n",
      "|-1736616907|133298243462499|Sorveteria Mr. Fr...|Ice Cream Shop, F...|          2|Rua Nove de Julho...|    Vinhedo| SP| 355670116|2019-03-14 02:08:27|    null|\n",
      "|-1002959781|176326925911652|Churrascaria Da F...|Cafeteria, Barbec...|          2|Rodovia do Açúcar...|      Salto| SP|      null|2019-03-14 02:08:27|    null|\n",
      "|-1099708153|193288280743387|  pizza kone express|Brazilian Restaur...|          3|Rua das Pitanguei...|    Jundiaí| SP|      null|2019-03-14 02:08:27|    null|\n",
      "|-1677672953|250585885104261|     Thais Kit Festa|Dessert Shop, Sho...|          1|Rua henrique feli...|Nova Odessa| SP|3533403014|2019-03-14 02:08:27|    null|\n",
      "+-----------+---------------+--------------------+--------------------+-----------+--------------------+-----------+---+----------+-------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#spark.sql('truncate table bairro')\n",
    "expired_conc = spark.sql('select id, codigo, nome, categoria, faixa_preco, endereco, municipio, uf, \\\n",
    "    bairro_id, start_date, from_unixtime(unix_timestamp()) as end_date \\\n",
    "    from old_conc WHERE has_changed=1')\n",
    "\n",
    "new_conc = spark.sql('select IF(NVL(b.has_changed, 1)==1,hash(a.codigo, unix_timestamp()),b.id) as id, \\\n",
    "    a.codigo, a.nome, a.categoria, a.faixa_preco, a.endereco, a.municipio, a.uf, a.bairro_id, \\\n",
    "    IF(NVL(b.has_changed, 1)==1, from_unixtime(unix_timestamp()), b.start_date) as start_date, \\\n",
    "    null as end_date FROM raw_conc as a LEFT join old_conc as b ON a.codigo=b.codigo')\n",
    "\n",
    "new_conc = new_conc.union(expired_conc)\n",
    "print('Contagem de registros:', new_conc.count())\n",
    "new_conc.show(5)\n",
    "new_conc.registerTempTable('new_conc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregamento da dimensão CONCORRENTE\n",
    "- Nesta etapa, todos os registros do warehouse são combinados com os novos registros em uma operação de *FULL JOIN*\n",
    "- Então, para cada atributo, é escolhido o primeiro valor não nulo, iniciando pelos novos registros\n",
    "- Finalmente, os dados são carregados em uma tabela *buffer*, a tabela original é removida, e a tabela buffer é renomeada como definitiva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contagem de registros: 4202\n",
      "+-----------+----------------+--------------------+--------------------+-----------+--------------------+---------+-------------------+--------+---+---------+\n",
      "|         id|          codigo|                nome|           categoria|faixa_preco|            endereco|bairro_id|         start_date|end_date| uf|municipio|\n",
      "+-----------+----------------+--------------------+--------------------+-----------+--------------------+---------+-------------------+--------+---+---------+\n",
      "|-1750629432| 442833619100677|          Duo Festas|Food & Beverage, ...|          0|  Campinas, Campinas| 35095072|2019-03-14 02:09:18|    null| SP| Campinas|\n",
      "|-1021758227| 116732941852794|     Hondai Japanese|    Sushi Restaurant|          2|Avenida Guilherme...| 35095070|2019-03-14 02:09:18|    null| SP| Campinas|\n",
      "| -670447084|1537292439871511|Doces e Salgados ...|Grocery Store, Ca...|          0|Rua 18, 338, 1305...| 35095090|2019-03-14 02:09:18|    null| SP| Campinas|\n",
      "| -433556087| 185621464833105|      Estação Moinho|Bar, Brewery, Dan...|          2|Rua Orlando Pauli...| 35095076|2019-03-14 02:09:18|    null| SP| Campinas|\n",
      "| -333992687| 555424814493577| Pizzarella Campinas|Pizza Place, Groc...|          1|RUA ANTONIO MARQU...| 35095072|2019-03-14 02:09:18|    null| SP| Campinas|\n",
      "+-----------+----------------+--------------------+--------------------+-----------+--------------------+---------+-------------------+--------+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_conc = spark.sql('select NVL(a.id, b.id) as id, NVL(a.codigo, b.codigo) as codigo, \\\n",
    "    NVL(a.nome, b.nome) as nome, NVL(a.categoria, b.categoria) as categoria, \\\n",
    "    NVL(a.faixa_preco, b.faixa_preco) as faixa_preco, \\\n",
    "    NVL(a.endereco, b.endereco) as endereco, \\\n",
    "    NVL(a.bairro_id, b.bairro_id) as bairro_id, \\\n",
    "    NVL(a.start_date, b.start_date) as start_date, NVL(a.end_date, b.end_date) as end_date, \\\n",
    "    NVL(a.uf, b.uf) as uf, NVL(a.municipio, b.municipio) as municipio \\\n",
    "    FROM new_conc as a FULL join concorrente as b ON a.id=b.id')\n",
    "\n",
    "final_conc.registerTempTable('final_conc')\n",
    "print('Contagem de registros:', final_conc.count())\n",
    "\n",
    "spark.sql('SET hive.exec.dynamic.partition=true')\n",
    "spark.sql('SET hive.exec.dynamic.partition.mode=nonstrict')\n",
    "\n",
    "spark.sql('create table conc_buff like concorrente')\n",
    "spark.sql('insert into table conc_buff SELECT * from final_conc')\n",
    "spark.sql('drop table concorrente')\n",
    "spark.sql('alter table conc_buff RENAME TO concorrente')\n",
    "spark.sql('select * from concorrente limit 5').show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
